{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of iterations\n",
    "T = 1000\n",
    "\n",
    "# Eta in the paper\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Let w = weight matrix\n",
    "# Let f_w = neural network, input: single feature vector, output: score\n",
    "\n",
    "# Let f_w(x[i][j]) = score of \n",
    "\n",
    "# Given query q[i], the ranking function f_w can generate a score list z[i]\n",
    "# Let z[i](f_w) = [ f_w(x[i][1]), f_w(x[i][2]), f_w(x[i][3]), ... f_w(x[i][n]) ]\n",
    "\n",
    "# Probability of documents 1 .. K is:\n",
    "# P_z[i](f_w) ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PermutationGeneratorModel(object):\n",
    "    \"\"\"Wrapper for Tensorflow model graph for generating permutations of feature vector lists\"\"\"\n",
    "    \n",
    "    def __init__(self, num_inputs):\n",
    "        # create graph\n",
    "        self._num_inputs = num_imputs\n",
    "        \n",
    "    def run_train_step(self, sess, list_batch, judgments, list_lens, loss_weights):\n",
    "        to_return = [self._train_op, self._summaries, self._loss, self.global_step]\n",
    "        return sess.run(to_return,\n",
    "                       feed_dict{self._lists: list_batch,\n",
    "                                 self._list_lens: list_lens})\n",
    "    \n",
    "    def run_eval_step(self, sess, list_batch, judgments, list_lens, loss_weights):\n",
    "        to_return = [self._summaries, self._loss, self.global_step]\n",
    "        return sess.run(to_return,\n",
    "                       feed_dict{self._lists: list_batch,\n",
    "                                 self._judgments: judgments,\n",
    "                                 self._list_lens: list_lens,\n",
    "                                 self._loss_weights: loss_weights})\n",
    "    \n",
    "    def run_predict_step(self, sess, list_batch, judgments, list_lens, loss_weights):\n",
    "        to_return = [self._outputs, self.global_step]\n",
    "        return sess.run(to_return,\n",
    "                       feed_dict={self._lists: list_batch,\n",
    "                                 self._judgments: judgments,\n",
    "                                 self._list_lens: list_lens,\n",
    "                                 self._loss_weights: loss_weights})    \n",
    "\n",
    "    def _add_placeholders(self):\n",
    "        \"\"\"Inputs which will be fed to the graph\"\"\"\n",
    "        hps = self._hps\n",
    "        self._lists = tf.placeholder(tf.float32,\n",
    "                                     [hps.batch_size, hps.max_list_length],\n",
    "                                     name='lists')\n",
    "        self._judgments: tf.placeholder(tf.int32,\n",
    "                                       [hps.batch_size, hps.max_list_length],\n",
    "                                       name='judgments')\n",
    "        self._list_lens = tf.placeholder(tf.int32, [hps.batch_size], name='list_lengths')\n",
    "        self._loss_weights = tf.placeholder(tf.float32, \n",
    "                                            [hps.batch_size, hps.max_list_length]\n",
    "                                            name='loss_weights')\n",
    "        \n",
    "    def _linear_layer(shape, inputs):\n",
    "        initial_weight = tf.truncated_normal(shape, stddev=0.1)\n",
    "        initial_bias = tf.constant(0.1, shape=shape)\n",
    "        \n",
    "        weight = tf.Variable(initial_weight)\n",
    "        bias = tf.Variable(initial_bias)\n",
    "        return weight * inputs + bias\n",
    "        \n",
    "    def _add_permutation_generator(self):\n",
    "        hps = self._hps\n",
    "        \n",
    "        with tf.variable_scope('generator'):\n",
    "            lists = self._lists\n",
    "            judgments = self._judgments\n",
    "            list_lens = self._list_lens\n",
    "            loss_weights = self._loss_weights\n",
    "            \n",
    "            inputs = lists\n",
    "            for i in xrange(hps.layers):\n",
    "                with tf.variable_scope('layer_%d' % i), \n",
    "                     tf.device(self._next_device):\n",
    "                    linear_layer = _linear_layer([hps.batch_size, hps.max_input_length], inputs)\n",
    "                    outputs = tf.nn.relu(linear_layer)\n",
    "                    inputs = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Information Retrieval metrics\n",
    "Useful Resources:\n",
    "http://www.cs.utexas.edu/~mooney/ir-course/slides/Evaluation.ppt\n",
    "http://www.nii.ac.jp/TechReports/05-014E.pdf\n",
    "http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "http://hal.archives-ouvertes.fr/docs/00/72/67/60/PDF/07-busa-fekete.pdf\n",
    "Learning to Rank for Information Retrieval (Tie-Yan Liu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def mean_reciprocal_rank(rs):\n",
    "    \"\"\"Score is reciprocal of the rank of the first relevant item\n",
    "    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
    "    >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.61111111111111105\n",
    "    >>> rs = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.5\n",
    "    >>> rs = [[0, 0, 0, 1], [1, 0, 0], [1, 0, 0]]\n",
    "    >>> mean_reciprocal_rank(rs)\n",
    "    0.75\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean reciprocal rank\n",
    "    \"\"\"\n",
    "    rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
    "    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])\n",
    "\n",
    "\n",
    "def r_precision(r):\n",
    "    \"\"\"Score is precision after all relevant documents have been retrieved\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> r = [0, 0, 1]\n",
    "    >>> r_precision(r)\n",
    "    0.33333333333333331\n",
    "    >>> r = [0, 1, 0]\n",
    "    >>> r_precision(r)\n",
    "    0.5\n",
    "    >>> r = [1, 0, 0]\n",
    "    >>> r_precision(r)\n",
    "    1.0\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        R Precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    z = r.nonzero()[0]\n",
    "    if not z.size:\n",
    "        return 0.\n",
    "    return np.mean(r[:z[-1] + 1])\n",
    "\n",
    "\n",
    "def precision_at_k(r, k):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> r = [0, 0, 1]\n",
    "    >>> precision_at_k(r, 1)\n",
    "    0.0\n",
    "    >>> precision_at_k(r, 2)\n",
    "    0.0\n",
    "    >>> precision_at_k(r, 3)\n",
    "    0.33333333333333331\n",
    "    >>> precision_at_k(r, 4)\n",
    "    Traceback (most recent call last):\n",
    "        File \"<stdin>\", line 1, in ?\n",
    "    ValueError: Relevance score length < k\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    if r.size != k:\n",
    "        raise ValueError('Relevance score length < k')\n",
    "    return np.mean(r)\n",
    "\n",
    "\n",
    "def average_precision(r):\n",
    "    \"\"\"Score is average precision (area under PR curve)\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
    "    >>> delta_r = 1. / sum(r)\n",
    "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
    "    0.7833333333333333\n",
    "    >>> average_precision(r)\n",
    "    0.78333333333333333\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Average precision\n",
    "    \"\"\"\n",
    "    r = np.asarray(r) != 0\n",
    "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
    "    if not out:\n",
    "        return 0.\n",
    "    return np.mean(out)\n",
    "\n",
    "\n",
    "def mean_average_precision(rs):\n",
    "    \"\"\"Score is mean average precision\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1]]\n",
    "    >>> mean_average_precision(rs)\n",
    "    0.78333333333333333\n",
    "    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [0]]\n",
    "    >>> mean_average_precision(rs)\n",
    "    0.39166666666666666\n",
    "    Args:\n",
    "        rs: Iterator of relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean average precision\n",
    "    \"\"\"\n",
    "    return np.mean([average_precision(r) for r in rs])\n",
    "\n",
    "\n",
    "def dcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> dcg_at_k(r, 1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 1, method=1)\n",
    "    3.0\n",
    "    >>> dcg_at_k(r, 2)\n",
    "    5.0\n",
    "    >>> dcg_at_k(r, 2, method=1)\n",
    "    4.2618595071429155\n",
    "    >>> dcg_at_k(r, 10)\n",
    "    9.6051177391888114\n",
    "    >>> dcg_at_k(r, 11)\n",
    "    9.6051177391888114\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Discounted cumulative gain\n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=0):\n",
    "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
    "    Relevance is positive real values.  Can use binary\n",
    "    as the previous methods.\n",
    "    Example from\n",
    "    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf\n",
    "    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "    >>> ndcg_at_k(r, 1)\n",
    "    1.0\n",
    "    >>> r = [2, 1, 2, 0]\n",
    "    >>> ndcg_at_k(r, 4)\n",
    "    0.9203032077642922\n",
    "    >>> ndcg_at_k(r, 4, method=1)\n",
    "    0.96519546960144276\n",
    "    >>> ndcg_at_k([0], 1)\n",
    "    0.0\n",
    "    >>> ndcg_at_k([1], 2)\n",
    "    1.0\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n",
    "    Returns:\n",
    "        Normalized discounted cumulative gain\n",
    "    \"\"\"\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches = 599\n",
      "Number of batches = 199\n",
      "Indices Tensor(\"GAN/decider/sort:1\", shape=(10, 128), dtype=int32)\n",
      "Indices dim 1 Tensor(\"GAN/decider/stack:0\", shape=(10, 128, 1), dtype=int32)\n",
      "Slice indices Tensor(\"GAN/decider/full_indices:0\", shape=(10, 128, 2), dtype=int32)\n",
      "Sorted rolled scores Tensor(\"GAN/decider/sorted_scores:0\", shape=(10, 128), dtype=float32)\n",
      "Rolled vectors Tensor(\"GAN/decider/x_vectors:0\", shape=(10, 128, 136), dtype=float32)\n",
      "Sorted rolled vectors Tensor(\"GAN/decider/sorted_vectors:0\", shape=(10, 128, 136), dtype=float32)\n",
      "Expanded sorted rolled vectors Tensor(\"GAN/decider/ExpandDims_1:0\", shape=(10, 128, 1), dtype=float32)\n",
      "Rolled input Tensor(\"GAN/decider/rolled_input:0\", shape=(10, 128, 137), dtype=float32)\n",
      "Indices Tensor(\"GAN/decider/sort_1:1\", shape=(10, 128), dtype=int32)\n",
      "Indices dim 1 Tensor(\"GAN/decider/stack_1:0\", shape=(10, 128, 1), dtype=int32)\n",
      "Slice indices Tensor(\"GAN/decider/full_indices_1:0\", shape=(10, 128, 2), dtype=int32)\n",
      "Sorted rolled scores Tensor(\"GAN/decider/sorted_scores_1:0\", shape=(10, 128), dtype=float32)\n",
      "Rolled vectors Tensor(\"GAN/z_rolled_vectors:0\", shape=(10, 128, 136), dtype=float32)\n",
      "Sorted rolled vectors Tensor(\"GAN/decider/sorted_vectors_1:0\", shape=(10, 128, 136), dtype=float32)\n",
      "Expanded sorted rolled vectors Tensor(\"GAN/decider/ExpandDims_3:0\", shape=(10, 128, 1), dtype=float32)\n",
      "Rolled input Tensor(\"GAN/decider/rolled_input_1:0\", shape=(10, 128, 137), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from collections import namedtuple\n",
    "from itertools import izip\n",
    "from math import log\n",
    "from tabulate import tabulate\n",
    "import random\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "hps = namedtuple('Params',\n",
    "    'batch_size, max_list_length, vector_size, max_value, d_rnn_dim, d_relu_dim, g_layer2_dim, g_relu_dim'\n",
    ")\n",
    "\n",
    "hps.batch_size = 10\n",
    "hps.max_list_length = 128\n",
    "hps.d_rnn_dim = 64\n",
    "hps.d_relu_dim = 8\n",
    "hps.g_layer2_dim = 100\n",
    "hps.g_relu_dim = 10\n",
    "\n",
    "mu, sigma = 0, 1\n",
    "vectors_train, scores_train, _ = group_into_batches(X_train, y_train, qid_train, hps.batch_size, hps.max_list_length, hps.vector_size)\n",
    "vectors_devel, scores_devel, _ = group_into_batches(X_devel, y_devel, qid_train, hps.batch_size, hps.max_list_length, hps.vector_size)\n",
    "hps.max_value = scores_train.max()\n",
    "hps.vector_size = 136\n",
    "\n",
    "def clamp_as_probability(input):\n",
    "    return tf.maximum(tf.minimum(input, .99), .01)\n",
    "\n",
    "def make_decider(rolled_vectors, rolled_scores):\n",
    "    _, indices = tf.nn.top_k(rolled_scores, k=hps.max_list_length, sorted=True, name=\"sort\")\n",
    "    \n",
    "    rows = []\n",
    "    for i in xrange(hps.batch_size):\n",
    "        row = tf.fill([hps.max_list_length, 1], i)\n",
    "        rows.append(row)\n",
    "    indices_dim_1 = tf.stack(rows)\n",
    "    print \"Indices\", indices\n",
    "    print \"Indices dim 1\", indices_dim_1\n",
    "    \n",
    "    slice_indices = tf.concat(2, [indices_dim_1, tf.expand_dims(indices, -1)], name=\"full_indices\")\n",
    "    print \"Slice indices\", slice_indices\n",
    "    #print slice_indices\n",
    "    \n",
    "    #print rolled_scores\n",
    "    # Unroll vectors and scores\n",
    "    sorted_rolled_scores = tf.gather_nd(rolled_scores, slice_indices, 'sorted_scores')\n",
    "    print \"Sorted rolled scores\", sorted_rolled_scores\n",
    "    #print sorted_rolled_scores\n",
    "    \n",
    "    print \"Rolled vectors\", rolled_vectors\n",
    "    sorted_rolled_vectors = tf.gather_nd(rolled_vectors, slice_indices, 'sorted_vectors')\n",
    "    print \"Sorted rolled vectors\", sorted_rolled_vectors\n",
    "    #print sorted_rolled_vectors\n",
    "\n",
    "    expanded_sorted_rolled_scores = tf.expand_dims(sorted_rolled_scores, -1)\n",
    "    print \"Expanded sorted rolled vectors\", expanded_sorted_rolled_scores\n",
    "    rolled_input = tf.concat(2, [sorted_rolled_vectors, expanded_sorted_rolled_scores], name='rolled_input')\n",
    "    print \"Rolled input\", rolled_input\n",
    "    unrolled_input = tf.unstack(rolled_input, axis=1)\n",
    "    \n",
    "    gru = tf.nn.rnn_cell.GRUCell(hps.d_rnn_dim)\n",
    "    outputs, state = tf.nn.rnn(gru, unrolled_input, scope=\"rnn\", dtype=tf.float32)\n",
    "    \n",
    "    last_rnn_output = outputs[-1]\n",
    "    \n",
    "    w1=tf.get_variable(\"d_w0\", [hps.d_rnn_dim, hps.d_relu_dim], initializer=tf.random_uniform_initializer())\n",
    "    b1=tf.get_variable(\"d_b0\", [hps.d_relu_dim], initializer=tf.constant_initializer(0.1))\n",
    "    w2=tf.get_variable(\"d_w1\", [hps.d_relu_dim, 1], initializer=tf.random_normal_initializer())\n",
    "    b2=tf.get_variable(\"d_b1\", [1], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "    layer2 = tf.nn.relu(tf.batch_matmul(last_rnn_output, w1) + b1)\n",
    "    layer3 = tf.sigmoid(tf.batch_matmul(layer2, w2) + b2)\n",
    "    return clamp_as_probability(layer3)\n",
    "\n",
    "# [BATCH_SIZE] x [LIST_LENGTH] x [VECTOR_SIZE]\n",
    "with tf.variable_scope(\"GAN\"):\n",
    "    z_seeds = tf.placeholder(tf.float32,\n",
    "                            [hps.batch_size, 1],\n",
    "                            name='z_seed')\n",
    "    z_rolled_vectors = tf.placeholder(tf.float32,\n",
    "                                      [hps.batch_size, hps.max_list_length, hps.vector_size],\n",
    "                                      name='z_rolled_vectors')\n",
    "    with tf.variable_scope(\"generator\") as scope:\n",
    "        w1=tf.get_variable(\"g_w0\", [hps.vector_size + 1, hps.g_relu_dim], initializer=tf.random_uniform_initializer())\n",
    "        b1=tf.get_variable(\"g_b0\", [hps.g_relu_dim], initializer=tf.constant_initializer(0.1))\n",
    "        w2=tf.get_variable(\"g_w1\", [hps.g_relu_dim, hps.g_layer_2_dim], initializer=tf.random_normal_initializer())\n",
    "        b2=tf.get_variable(\"g_b1\", [hps.g_layer_2_dim], initializer=tf.constant_initializer(0.1))\n",
    "        w2=tf.get_variable(\"g_w2\", [hps.g_layer_2_dim, 1], initializer=tf.random_normal_initializer())\n",
    "        b2=tf.get_variable(\"g_b2\", [1], initializer=tf.constant_initializer(0.1))\n",
    "    \n",
    "        # List of [BATCH_SIZE] x [VECTOR_SIZE]\n",
    "        z_vectors = tf.unstack(z_rolled_vectors, axis=1)\n",
    "        z_scores = []\n",
    "        for z_vector in z_vectors:\n",
    "            #print z_vector\n",
    "            #print z_seeds\n",
    "            z_vector_with_seed = tf.concat(1, [z_vector, z_seeds])\n",
    "            g_layer_1 = tf.nn.relu(tf.batch_matmul(z_vector_with_seed, w1) + b1)\n",
    "            g_layer_2 = tf.nn.relu(tf.batch_matmul(g_layer_1, w2) + b2)\n",
    "            z_score = tf.sigmoid(tf.batch_matmul(g_layer_1, w3) + b3)\n",
    "            z_score = clamp_as_probability(z_score) * max_value\n",
    "            z_score = tf.squeeze(z_score)\n",
    "            z_scores.append(z_score)\n",
    "\n",
    "        z_scores = tf.stack(z_scores, axis=1, name='z_scores')\n",
    "        theta_g = [v for v in tf.global_variables() if v.name.startswith(scope.name)]\n",
    "\n",
    "    with tf.variable_scope(\"decider\") as scope:\n",
    "        x_vectors = tf.placeholder(tf.float32,\n",
    "                   [hps.batch_size, hps.max_list_length, hps.vector_size],\n",
    "                   name='x_vectors')\n",
    "        x_scores = tf.placeholder(tf.float32,\n",
    "                            [hps.batch_size, hps.max_list_length],\n",
    "                            name='x_scores')\n",
    "        \n",
    "        D1 = make_decider(x_vectors, x_scores)\n",
    "        \n",
    "        scope.reuse_variables()\n",
    "        \n",
    "        D2 = make_decider(z_rolled_vectors, z_scores)\n",
    "        \n",
    "        theta_d = [v for v in tf.global_variables() if v.name.startswith(scope.name)]\n",
    "       \n",
    "  \n",
    "    step_d = tf.Variable(0)\n",
    "    obj_d = tf.reduce_mean(tf.log(D1) + tf.log(1 - D2))\n",
    "    opt_d = tf.train.GradientDescentOptimizer(0.01).minimize(1 - obj_d, global_step=step_d, var_list=theta_d)\n",
    "\n",
    "    step_g = tf.Variable(0)\n",
    "    obj_g = tf.reduce_mean(tf.log(D2))\n",
    "    opt_g = tf.train.GradientDescentOptimizer(0.01).minimize(1 - obj_g, global_step=step_g, var_list=theta_g)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "   \n",
    "    def evaluate_list(estimated_scores, true_scores):\n",
    "        #print \"Estimated\", len(estimated_scores), \"==\", estimated_scores\n",
    "        #print \"True\", len(true_scores), \"==\", true_scores\n",
    "        rs = [x[1] for x in sorted(izip(estimated_scores, true_scores), key=lambda x: -x[0])]\n",
    "        #print rs\n",
    "        ndcg_10  = ndcg_at_k(rs, 10, method=1)\n",
    "        ndcg_100 = ndcg_at_k(rs, 100, method=1)\n",
    "        p_10 = precision_at_k(rs, 10)\n",
    "        m_ap = average_precision(rs)\n",
    "        return [ndcg_10, ndcg_100, p_10, m_ap]\n",
    "    \n",
    "    def evaluate_lists(guess_lists, gold_lists):\n",
    "        evaluations = []\n",
    "        for guess, gold in izip(guess_lists, gold_lists):\n",
    "            evaluations.append(evaluate_list(guess, gold))\n",
    "        return np.mean(evaluations, axis=0)    \n",
    "    \n",
    "    # Algorithm 1, GoodFellow et al. 2014\n",
    "    for epoch in range(10000):\n",
    "        d1s = []\n",
    "        d1_losses = []\n",
    "        d2s = []\n",
    "        d2_losses = []\n",
    "        train_guess_scores = []\n",
    "        train_gold_scores = []\n",
    "        start = timer()\n",
    "        for i in xrange(0, len(vectors_train) - 3, 3):\n",
    "            _x_vectors = np.asarray(vectors_train[i])\n",
    "            _x_scores = np.asarray(scores_train[i])\n",
    "            _z_vectors = np.asarray(vectors_train[i + 1])\n",
    "            _z_seeds = np.random.normal(mu, sigma, (hps.batch_size, 1))\n",
    "            d1, d2, o1, _ = sess.run([D1, D2, obj_d, opt_d], {x_vectors: _x_vectors, x_scores: _x_scores, z_rolled_vectors: _z_vectors, z_seeds: _z_seeds})\n",
    "            d1s.append(np.mean(d1))\n",
    "            d1_losses.append(np.mean(1 - o1))\n",
    "            _z_vectors = np.asarray(vectors_train[i + 2])\n",
    "            _z_seeds = np.random.normal(mu, sigma, (hps.batch_size, 1))\n",
    "            d2, scores, o2, _ = sess.run([D2, z_scores, obj_g, opt_g], {x_vectors: _x_vectors, x_scores: _x_scores, z_rolled_vectors: _z_vectors, z_seeds: _z_seeds})\n",
    "            d2s.append(np.mean(d2))\n",
    "            d2_losses.append(np.mean(1 - o2))\n",
    "            train_guess_scores.extend(scores)\n",
    "            train_gold_scores.extend(scores_train[i + 2])\n",
    "        d1 = np.mean(d1s)\n",
    "        d2 = np.mean(d2s)\n",
    "        d1_loss = np.mean(d1_losses)\n",
    "        d2_loss = np.mean(d2_losses)\n",
    "        end = timer()\n",
    "        print \"Epoch %d: Time: %s; D1=%6.3f (L1=%6.3f); D2=%6.3f (L2=%6.3f)\" % (epoch + 1, end - start, d1, 1 - d1_loss, d2, 1 - d2_loss)\n",
    "        devel_guess_scores = []\n",
    "        devel_gold_scores = []\n",
    "        for i in xrange(len(vectors_devel)):\n",
    "            _x_vectors = _z_rolled_vectors = np.asarray(vectors_devel[i])\n",
    "            _x_scores = _z_gold_scores = scores_devel[i]\n",
    "            _z_seeds = np.random.normal(mu, sigma, (hps.batch_size, 1))\n",
    "            _z_guess_scores = sess.run([z_scores], {x_vectors: _x_vectors, x_scores: np.asarray(_x_scores), z_rolled_vectors: _z_vectors, z_seeds: _z_seeds})\n",
    "            devel_guess_scores.extend(_z_guess_scores[0])\n",
    "            devel_gold_scores.extend(_x_scores)\n",
    "        train_evals = evaluate_lists(train_guess_scores, train_gold_scores).tolist()\n",
    "        train_evals.insert(0, 'Train')\n",
    "        #print \"Devel guess scores\", devel_guess_scores\n",
    "        #print \"Devel gold scores\", devel_gold_scores\n",
    "        devel_evals = evaluate_lists(devel_guess_scores, devel_gold_scores).tolist()\n",
    "        devel_evals.insert(0, 'Devel')\n",
    "        print tabulate([train_evals, devel_evals], headers=['', 'NDCG@10', 'NDCG@100', 'P@10', 'MAP'], tablefmt='fancy_grid')\n",
    "        \n",
    "        t = list(zip(vectors_train, scores_train))\n",
    "        random.shuffle(t)\n",
    "        vectors_train, scores_train = zip(*t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SVMLight file in 116.278 seconds\n",
      "Converted to flat numpy arrays in  1.569 seconds\n",
      "Bucketed SVMLight vectors in  0.147 seconds\n",
      "Loaded SVMLight file in 37.663 seconds\n",
      "Converted to flat numpy arrays in  0.514 seconds\n",
      "Bucketed SVMLight vectors in  0.044 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "\n",
    "# Data parsing\n",
    "def load_letor_data(path):\n",
    "    # Initially, vectors are flat\n",
    "    start = timer()\n",
    "    X_flat, y_flat, qid_flat = load_svmlight_file(path, dtype=np.float32, query_id=True)\n",
    "    end = timer()\n",
    "    print \"Loaded SVMLight file in %6.3f seconds\" % (end - start)\n",
    "    \n",
    "    # Flatten to numpy arrays\n",
    "    start = timer()\n",
    "    X_flat = X_flat.toarray()\n",
    "    y_flat\n",
    "    qid_flat\n",
    "    end = timer()\n",
    "    print \"Converted to flat numpy arrays in %6.3f seconds\" % (end - start)\n",
    "    \n",
    "    # We need to group them into buckets, one for each qid\n",
    "    start = timer()\n",
    "    qids, index, counts = np.unique(qid_flat, return_index=True, return_counts=True)\n",
    "    n_qids = len(qids)\n",
    "    X = []\n",
    "    y = []\n",
    "    qid = []\n",
    "    for i in xrange(n_qids):\n",
    "        start_index = index[i]\n",
    "        end_index = index[i] + counts[i]\n",
    "        assert np.all(qid_flat[start_index:end_index] == qids[i]), \"Found QIDs in bucket[%d:%d] with mismatched QIDs = %s (should be %d)\" % (start_index, end_index, qid_flat[start_index:end_index], qids[i])\n",
    "        X.append(X_flat[start_index:end_index])\n",
    "        y.append(y_flat[start_index:end_index])\n",
    "        qid.append(qids[i])\n",
    "    end = timer()\n",
    "    print \"Bucketed SVMLight vectors in %6.3f seconds\"  % (end - start)\n",
    "    return np.array(X), np.array(y), np.array(qid)\n",
    "\n",
    "X_train, y_train, qid_train = load_letor_data('/shared/aifiles/disk1/travis/data/corpora/letor/letor_4/MSLR-WEB10K/Fold1/train.txt')    \n",
    "X_devel, y_devel, qid_devel = load_letor_data('/shared/aifiles/disk1/travis/data/corpora/letor/letor_4/MSLR-WEB10K/Fold1/vali.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to batches\n",
    "def group_into_batches(X, y, qid, batch_size, max_len, num_feats):\n",
    "    X_batched = []\n",
    "    y_batched = []\n",
    "    qid_batched = []\n",
    "    for i in xrange(0, len(X) - batch_size, batch_size):\n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "        qid_batch = qid[i:i+batch_size]\n",
    "        for j in xrange(batch_size):\n",
    "            _X = np.zeros([max_len, num_feats], dtype=np.float32)\n",
    "            _y = np.zeros([max_len], dtype=np.float32)\n",
    "            n = min(X[i + j].shape[0], max_len)\n",
    "            assert X[i + j].shape[0] == y[i + j].shape[0]\n",
    "            _X[0:n] = X[i + j][0:n]\n",
    "            _y[0:n] = y[i + j][0:n]\n",
    "            X_batch.append(_X)\n",
    "            y_batch.append(_y)\n",
    "        X_batched.append(X_batch)\n",
    "        y_batched.append(y_batch)\n",
    "        qid_batched.append(qid_batch)\n",
    "    num_batches = len(X_batched)\n",
    "    assert len(X_batched) == len(y_batched) == len(qid_batched)\n",
    "    print \"Number of batches = %d\" % num_batches\n",
    "    X_batched = np.asarray(X_batched)\n",
    "    y_batched = np.asarray(y_batched)\n",
    "    qid_batched = np.asarray(qid_batched)\n",
    "#    X_batched.resize(num_batches, batch_size, max_len, num_feats)\n",
    "#    y_batched.resize(num_batches, batch_size, max_len)\n",
    "    return X_batched, y_batched, qid_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_soft_qids(qids):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_.shape\n",
    "print y_.shape\n",
    "print qid_.shape\n",
    "\n",
    "print \"X:\", X_.shape\n",
    "print \"1st batch of X\", X_[0].shape\n",
    "print \"1st sample of 1st batch of X\", X_[0][0].shape\n",
    "print \"1st document in 1st sample of 1st batch of X\", X_[0][0][0].shape\n",
    "print \"1st feature in 1st document in 1st sample of 1st batch of X\", X_[0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}